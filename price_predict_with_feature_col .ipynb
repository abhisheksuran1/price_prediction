{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Real estate.csv\")\n",
    "df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.drop('No', axis=1,inplace=True)\n",
    "df.transactionDate = df['transactionDate'].astype('int')\n",
    "df.transactionDate = df['transactionDate'].astype('str')\n",
    "df.latitude = df['latitude'].astype('str')\n",
    "df.longitude = df['longitude'].astype('str')\n",
    "df = df.dropna()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "                                  dataframe = dataframe.copy()\n",
    "                                  labels = dataframe.pop('house_price_of_unit_area')\n",
    "                                  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "                                  if shuffle:\n",
    "                                    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "                                  ds = ds.batch(batch_size)\n",
    "                                  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "              print('Every feature:', list(feature_batch.keys()))\n",
    "              print('A batch of', feature_batch['houseAge'])\n",
    "              print('A batch of targets:', label_batch )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature columns\n",
    "\n",
    "num_col = ['houseAge', 'distanceToTheNearestMRTStation', 'number_of_convenience_stores'] #numeric columns\n",
    "buk_col = ['number_of_convenience_stores', 'houseAge'] # bucketised columns\n",
    "cate_col = ['transactionDate'] # categorical\n",
    "emb_col = ['latitude', 'longitude'] # embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scal(feature):\n",
    "      def minmax(x):\n",
    "        mini = train[feature].min()\n",
    "        maxi = train[feature].max()\n",
    "        return (x - mini)/(maxi-mini)\n",
    "      return(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for col in num_col:\n",
    "          scal_input_fn = get_scal(col)\n",
    "          feature_columns.append(feature_column.numeric_column(col, normalizer_fn=scal_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "houseAge = feature_column.numeric_column(\"houseAge\")\n",
    "# bucketized cols\n",
    "houseAge_bucket = feature_column.bucketized_column(houseAge, boundaries=[0,9,18,27,36,45])\n",
    "feature_columns.append(houseAge_bucket)\n",
    "\n",
    "\n",
    "\n",
    "number_of_convenience_stores = feature_column.numeric_column(\"number_of_convenience_stores\")\n",
    "# bucketized cols\n",
    "number_of_convenience_stores_buckets = feature_column.bucketized_column(number_of_convenience_stores, boundaries=[0, 2, 4, 6, 8, 10])\n",
    "feature_columns.append(number_of_convenience_stores_buckets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in cate_col:\n",
    "                  vocabulary = df[name].unique()\n",
    "                  cat_c = tf.feature_column.categorical_column_with_vocabulary_list(name, vocabulary)\n",
    "                  one_hot = feature_column.indicator_column(cat_c)\n",
    "                  feature_columns.append(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in emb_col:\n",
    "                  vocabulary = df[col_name].unique()\n",
    "                  cat_c = tf.feature_column.categorical_column_with_vocabulary_list(col_name, vocabulary)\n",
    "                  embeding = feature_column.embedding_column(cat_c, dimension=50)\n",
    "                  feature_columns.append(embeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "crossed_feature = feature_column.crossed_column([houseAge_bucket,number_of_convenience_stores_buckets], hash_bucket_size=1000)\n",
    "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "feature_columns.append(crossed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of feature coumns: ',len(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(16, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
    "  layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
    "  layers.Dropout(0.2),\n",
    "  \n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds,validation_data=val_ds,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
